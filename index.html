
<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Patrick H. Chen</title>
<meta name="description" content="PHC homepage
">

<!-- Open Graph -->
<link rel="stylesheet" href="github-markdown.css">


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />
>
<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> 

<!-- Styles -->
 
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
 

 <link rel="stylesheet" href="main.css"> 

<link rel="canonical" href="index.html">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-154898229-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-154898229-1');
  </script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


		<style>
			.markdown-body {
				box-sizing: border-box;
				min-width: 200px;
				max-width: 980px;
				margin: 0 auto;
				padding: 45px;
			}
			@media (max-width: 767px) {
				.markdown-body {
					padding: 15px;
				}
				.left {
			    width: 100%;
			  }
			  .right {
			    width: 100%;
			  }
				.portrait {
					margin-bottom: 1em;
			  }
				.column-third {
					float: left;
			    width: 100%;
			  }
			}
			html {
			  width: 100vw;
			}
			body {
			  overflow-x: hidden;
			}
		</style>



  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="index.html">
       Patrick H. Chen
      </a>
      
      
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="index.html">
              About
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="publication.html">
                Publications
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>






    <!-- Content -->


    <div class="container mt-5">
			<!-- <h2>Home &nbsp; &nbsp; <a href="./pubs/index.html">Publications</a></h2> -->
		        <h1>Chris J. Maddison</h1>
	 		<div class="row markdown-body">
				<div class="column-picture left">
		 			<img class="portrait-circular" src="pic/PC.png">
				</div>
				<div class="column-picture right">
					<p>Ph.D. Candidate<br/>
					Dept. of Computer Science<br/>
					UCLA</p>

					<p>patrickchen [at] g [dot] ucla [dot] edu</p>

					<p>
						<a href="./pubs/index.html">Publications</a>, <a href="https://scholar.google.com/citations?user=WjCG3owAAAAJ&hl=en">Google Scholar</a>, <a href="./bio.html">Biography</a>
					</p>

				</div>
			</div>

			<h2>Research</h2>

			<p>My goal is to understand and improve the algorithms that agents can use to learn from data and reason about their experience. This goal is typically framed through the language of statistics, and solved using algorithms for probabilistic inference or optimization.</p>

					<p><b>Representation learning</b> &nbsp;&nbsp;&nbsp;&nbsp; How we represent data affects how effectively we process and understand it. I am interested in learning useful and robust representations, with a particular interest in techniques that learn representations from pretext tasks. In recent work, we showed how to <a href="https://arxiv.org/abs/2106.10800">learn compressed representations of data</a> with performance guarantees on a large, possibly infinite, set of downstream tasks.
					</p>
					<p>
					<b>Learning with discrete structure</b> &nbsp;&nbsp;&nbsp;&nbsp; I am also motivated by applications to reasoning about data with discrete structure, such as integer programming or discrete reasoning tasks. Together with colleagues, we built the first artificial agent that plays the <a href="https://www.nature.com/articles/nature16961">board game Go at a superhuman level</a>, developed structured models of human-written <a href="http://proceedings.mlr.press/v32/maddison14.html">source code</a>, and designed <a href="https://arxiv.org/abs/2006.08063">relaxed gradient estimators</a> for models with structured latent variables.
					</p>
					<p>
					<b>Inference and optimization</b> &nbsp;&nbsp;&nbsp;&nbsp; Algorithms for Bayesian inference and optimization are the engines that drive machine learning. Although these two problems seem distinct, they have a lot of shared structure. I am interested in this interplay: we showed how to simulate from a probability distribution by <a href="https://papers.nips.cc/paper/5449-a-sampling">optimizing a random function</a> and how the use of <a href="https://epubs.siam.org/doi/abs/10.1137/19M130858X">a kinetic energy can condition optimization</a>.
					</p>



      <div class="post">


  <header class="post-header">
    
    
  </header>

  <article>
    <br/>
    <br/>
    <div class="clearfix">
      <h2 id="bio">Bio</h2>
<hr />
<p>I am a Ph.D. student in Computer Science at <a href="https://web.cs.toronto.edu/">University of Toronto</a>, supervised by <a href="http://www.cs.toronto.edu/~cmaddis/">Chris Maddison</a>. Previously, I obtained my Bachelor degree in Information Engineering from <a href="http://www.zju.edu.cn/english/">Zhejiang University</a> in July 2020. In summer 2019, I was a visiting student at <a href="https://www.cs.ucla.edu/">UCLA</a>, where I worked with <a href="http://web.cs.ucla.edu/~chohsieh/index.html#about">Cho-Jui Hsieh</a>.</p>

<p>I’m currently working on the intersection of machine learning and information theory, with a focus on representation learning, domain generalization, and neural compression. 
In particular, I want to understand how to learn useful and trustworthy representations from data, and motivate more efficient and principled representation learning methods.
I’m also interested in implicit deep learning and generative modeling.</p>



<p><br /></p>


                        <h2>Research</h2>

                        <p>My current research focsed on Efficient Machine Learning (EML). The goal of EML is to achieve a smaller model with fast inference speed on all devices. This includes studies of compressing machine learning models,approximating model inference operations, or acheving both simultaneoursly. In addition to EML, I am also keen on exploring fundamental problems of Machine Learning such as adversarial robustness or catastrophic forgetting. </p>




<h4 id="selected-publications-full-list"><strong>Selected Publications</strong> <a href="publication.html">[Full List]</a></h4>
<hr />
<p>* below denotes equal contribution</p>
<div class="publications">
  <ol class="bibliography"><li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div> -->

  <div id="ruan2022optdom" class="col-sm-10">
    
      <div class="title">Optimal Representations for Covariate Shift</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan*</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Yann Dubois*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Chris J Maddison
                
              
            
          
        
      </div>

      <div class="periodical">
      
          
            <em>In International Conference on Learning Representations</em>
          
        (<b>ICLR</b>),
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://arxiv.org/pdf/2201.00057.pdf" target="_blank">PDF</a>]
      
    
    
    
    
      [<a href="https://github.com/ryoungj/optdom" target="_blank">Code</a>]
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are \textitexactly the set of \textitall representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two components. First, a representation must remain discriminative for the task, i.e., some predictor must be able to simultaneously minimize the source and target risk. Second, the representation’s marginal support needs to be the same across source and target. We make this practical by designing self-supervised learning methods that only use unlabelled data and augmentations to train robust representations. Our objectives achieve state-of-the-art results on DomainBed, and give insights into the robustness of recent methods, such as CLIP.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div> -->

  <div id="ruan2021mcbits" class="col-sm-10">
    
      <div class="title">Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan*</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Karen Ullrich*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Daniel Severo*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  James Townsend,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Ashish Khisti,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Arnaud Doucet,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Alireza Makhzani,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Chris J Maddison
                
              
            
          
        
      </div>

      <div class="periodical">
      
          
            <em>In International Conference on Machine Learning</em>
          
        (<b>ICML</b>),
      
      
        2021
      
      
        <b style="color:#B71C1C;">[Long talk]</b>
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://arxiv.org/pdf/2102.11086.pdf" target="_blank">PDF</a>]
      
    
    
    
    
      [<a href="https://github.com/ryoungj/mcbits" target="_blank">Code</a>]
    
    
      
      [<a href="/~yjruan/assets/pdf/poster/McBits_poster.pdf" target="_blank">Poster</a>]
      
    
    
      
      [<a href="/~yjruan/assets/pdf/slides/McBits_slides.pdf" target="_blank">Slides</a>]
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Latent variable models have been successfully applied in lossless compression with the bits-back coding algorithm. However, bits-back suffers from an increase in the bitrate equal to the KL divergence between the approximate posterior and the true posterior. In this paper, we show how to remove this gap asymptotically by deriving bits-back coding algorithms from tighter variational bounds. The key idea is to exploit extended space representations of Monte Carlo estimators of the marginal likelihood. Naively applied, our schemes would require more initial bits than the standard bits-back coder, but we show how to drastically reduce this additional cost with couplings in the latent space. When parallel architectures can be exploited, our coders can achieve better rates than bits-back with little additional cost. We demonstrate improved lossless compression rates in a variety of settings, especially in out-of-distribution or sequential data compression.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div> -->

  <div id="ruan2020zol2l" class="col-sm-10">
    
      <div class="title">Learning to Learn by Zeroth-Order Oracle</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Yuanhao Xiong,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Sashank Reddi,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Sanjiv Kumar,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Cho-Jui Hsieh
                
              
            
          
        
      </div>

      <div class="periodical">
      
          
            <em>In International Conference on Learning Representations</em>
          
        (<b>ICLR</b>),
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://openreview.net/pdf?id=ryxz8CVYDH" target="_blank">PDF</a>]
      
    
    
    
    
      [<a href="https://github.com/ryoungj/ZO-L2L" target="_blank">Code</a>]
    
    
    
      
      [<a href="/~yjruan/assets/pdf/slides/ZO-L2L_slides.pdf" target="_blank">Slides</a>]
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the learning to learn (L2L) framework, we cast the design of optimization algorithms as a machine learning problem and use deep neural networks to learn the update rules. In this paper, we extend the L2L framework to zeroth-order (ZO) optimization setting, where no explicit gradient information is available. Our learned optimizer, modeled as recurrent neural network (RNN), first approximates gradient by ZO gradient estimator and then produces parameter update utilizing the knowledge of previous iterations. To reduce high variance effect due to ZO gradient estimator, we further introduce another RNN to learn the Gaussian sampling rule and dynamically guide the query direction sampling. Our learned optimizer outperforms hand-designed algorithms in terms of convergence rate and final solution on both synthetic and practical ZO optimization tasks (in particular, the black-box adversarial attack task, which is one of the most widely used tasks of ZO optimization). We finally conduct extensive analytical experiments to demonstrate the effectiveness of our proposed optimizer.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

<p><br /></p>
<h4 id="services"><strong>Services</strong></h4>
<hr />
<ul>
  <li>Conference reviewer: NeurIPS (2020, 2021), ICLR (2021, 2022), ICML (2021)</li>
  <li>Workshop reviewer: NeurIPS DGMs Applications Workshop (2021)</li>
</ul>

<p><br /></p>
<h4 id="selected-awards--honors"><strong>Selected Awards &amp; Honors</strong></h4>
<hr />
<ul>
  <li>DiDi Gruduate Student Award, 2021</li>
  <li>Computer Science 50th Anniversary Graduate Scholarship, 2020.</li>
  <li>CHU Kochen Scholarship (highest honor at Zhejiang University), 2019.</li>
  <li>Top 10 students at Zhejiang University, 2019.</li>
  <li>Cross-disciplinary Scholars in Science and Technology (CSST), UCLA, 2019.</li>
  <li>National Scholarship (top 1.5%), 2017, 2018, 2019.</li>
  <li>Meritorious Winner, Interdisciplinary Contest in Modeling (ICM), 2018.</li>
</ul>

    </div>

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container" style="text-align: center;">
    <!-- &copy; Copyright 2022 Yangjun  Ruan. -->
    
    
    
    Last updated: February 23, 2022.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="common.js"></script>


</html>

