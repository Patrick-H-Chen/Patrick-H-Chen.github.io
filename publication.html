
<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Patrick H. Chen | Publications</title>
<meta name="description" content="Patrick HP">


<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<!-- 
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
 -->

<link rel="stylesheet" href="main.css">

<link rel="canonical" href="publication.html">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-154898229-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-154898229-1');
  </script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="index.html">
       Patrick H. Chen 
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="index.html">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="publication.html">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h2 class="post-title"><b>Publications</b></h2>
    <p class="post-description"></p>
  </header>

  <article>
    <p>* below denotes equal contribution</p>
<div class="publications">


  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">

</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">

  <div id="ruan2020zol2l" class="col-sm-10">
      <div class="title">Learning to Learn by Zeroth-Order Oracle</div>
      <div class="author">
                <em>Yangjun Ruan</em>,  
                  and Cho-Jui Hsieh
      </div>
      <div class="periodical">
            <em>In International Conference on Learning Representations</em>
        (<b>ICLR</b>),
        2020
      </div>
    <div class="links">
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="https://openreview.net/pdf?id=ryxz8CVYDH" target="_blank">PDF</a>]
      [<a href="https://github.com/ryoungj/ZO-L2L" target="_blank">Code</a>]
      [<a href="/~yjruan/assets/pdf/slides/ZO-L2L_slides.pdf" target="_blank">Slides</a>]
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>In the learning to learn (L2L) framework, we cast the design of optimization algorithms as a machine learning problem and use deep neural networks to learn the update rules. In this paper, we extend the L2L framework to zeroth-order (ZO) optimization setting, where no explicit gradient information is available. Our learned optimizer, modeled as recurrent neural network (RNN), first approximates gradient by ZO gradient estimator and then produces parameter update utilizing the knowledge of previous iterations. To reduce high variance effect due to ZO gradient estimator, we further introduce another RNN to learn the Gaussian sampling rule and dynamically guide the query direction sampling. Our learned optimizer outperforms hand-designed algorithms in terms of convergence rate and final solution on both synthetic and practical ZO optimization tasks (in particular, the black-box adversarial attack task, which is one of the most widely used tasks of ZO optimization). We finally conduct extensive analytical experiments to demonstrate the effectiveness of our proposed optimizer.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div> -->

  <div id="ren2019fastspeech" class="col-sm-10">
    
      <div class="title">FastSpeech: Fast, Robust and Controllable Text to Speech</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                
                  Yi Ren*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan*</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Xu Tan,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Tao Qin,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Sheng Zhao,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Zhou Zhao,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Tie-Yan Liu
                
              
            
          
        
      </div>

      <div class="periodical">
      
          
            <em>In Advances in Neural Information Processing Systems</em>
          
        (<b>NeurIPS</b>),
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://arxiv.org/pdf/1905.09263.pdf" target="_blank">PDF</a>]
      
    
    
    
    
    
      
      [<a href="/~yjruan/assets/pdf/poster/fastspeech_poster.pdf" target="_blank">Poster</a>]
      
    
    
    
    
      [<a href="https://speechresearch.github.io/fastspeech/" target="_blank">Demo</a>]
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (eg, Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (ie, some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
  </div> -->


  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div id="groupreduce18" class="col-sm-10">
    
      <div class="title">GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking</div>
      <div class="author">
        
        
        
              
                <em>Patrick H. Chen</em>,  
                Si Si,
                Yang Li,
                Ciprian Chelba
                and Cho-jui, Hsieh
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
          
        (<b>NeurIPS</b>),
      
      
        2018
        
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      <!--[<a href="https://ieeexplore.ieee.org/iel7/4234/5534602/08624521.pdf" target="_blank">PDF</a>] -->
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
    <p>
odel compression is essential for serving large deep neural nets on devices with
limited resources or applications that require real-time responses. As a case study, a
neural language model usually consists of one or more recurrent layers sandwiched
between an embedding layer used for representing input tokens and a softmax
layer for generating output tokens. For problems with a very large vocabulary
size, the embedding and the softmax matrices can account for more than half of
the model size. For instance, the bigLSTM model achieves great performance
on the One-Billion-Word (OBW) dataset with around 800k vocabulary, and its
word embedding and softmax matrices use more than 6GBytes space, and are
responsible for over 90% of the model parameters. In this paper, we propose
GroupReduce, a novel compression method for neural language models, based
on vocabulary-partition (block) based low-rank matrix approximation and the
inherent frequency distribution of tokens (the power-law distribution of words).
The experimental results show our method can significantly outperform traditional
compression methods such as low-rank approximation and pruning. On the OBW
dataset, our method achieved 6.6 times compression rate for the embedding and
softmax matrices, and when combined with quantization, our method can achieve
26 times compression rate, which translates to a factor of 12.8 times compression
for the entire model with very little degradation in perplexity.
</p>

    </div>
    
  </div>
</div>
</li></ol>






  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">

</div>
</li></ol>










</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container" style="text-align: center;">
    <!-- &copy; Copyright 2022 Yangjun  Ruan. -->
    
    
    
    Last updated: February 23, 2022.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="common.js"></script>


</html>

