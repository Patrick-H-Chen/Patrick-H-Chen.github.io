
<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Patrick H. Chen | Publications</title>
<meta name="description" content="Patrick HP">


<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<!-- 
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
 -->

<link rel="stylesheet" href="main.css">

<link rel="canonical" href="publication.html">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-154898229-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-154898229-1');
  </script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="index.html">
       Patrick H. Chen 
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="index.html">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="publication.html">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h2 class="post-title"><b>Publications</b></h2>
    <p class="post-description"></p>
  </header>

  <article>
    <p>* below denotes equal contribution</p>
<div class="publications">


<!--  <h2 class="year">2021</h2> -->
  <ol class="bibliography"><li><div class="row">

</div>
</li></ol>


  <!-- <div class="col-sm-2 abbr">
  
  </div> -->



  <h2>Fast Machine Learning Inference</h2>
  <hr/>
  <br/>



  <ol class="bibliography"><li><div class="row">
  <div id="finger23" class="col-sm-10">
      <div class="title">FINGER: Fast Inference for Graph-based Approximate Nearest Neighbor Search</div>

      <div class="author">
                <em>Patrick H. Chen</em>,  
                Wei-cheng Chang,
                Jyun-yu Jiang,
                Hsiang-fu Yu,
                and Cho-jui, Hsieh
      </div>
      <div class="periodical">

        <em>To Appear In International World Wide Web Conference</em>
          
        (<b>WWW</b>),
      
        2023
      </div>
        
    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="" ftarget="_blank">PDF</a>]
    
    </div>

    <div class="abstract hidden">
    <p>
Approximate K-Nearest Neighbor Search (AKNNS) has now become
ubiquitous in modern applications, such as a fast search procedure
with two-tower deep learning models. Graph-based methods for
AKNNS in particular have received great attention due to their
superior performance. These methods rely on greedy graph search
to traverse the data points as embedding vectors in a database. Under this greedy search scheme, we make a key observation: many
distance computations do not influence search updates so that these
computations can be approximated without hurting performance.
As a result, we propose FINGER, a fast inference method for efficient graph search in AKNNS. FINGER approximates the distance
function by estimating angles between neighboring residual vectors.
The approximated distance can be used to bypass unnecessary computations for faster searches. Empirically, when it comes to speeding
up the inference of HNSW, which is one of the most popular graph-based AKNNS methods, FINGER significantly outperforms existing
acceleration approaches and conventional libraries by 20% to 60% across different benchmark datasets.
    </p>

    </div>


   </div> 
  </div>
  </li></ol>

  <ol class="bibliography"><li><div class="row">
  <div id="elias22" class="col-sm-10">
      <div class="title">ELIAS: End-to-end Learning to Index and Search in Large Output Spaces</div>

      <div class="author">
                Nilesh Gupta,
                <em>Patrick H. Chen</em>,  
                Hsiang-fu Yu,
                Cho-jui, Hsieh,
                and Inderjit S. Dhillon
      </div>
      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
          
        (<b>NeurIPS</b>),
      
        2022
      </div>
        
    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="https://openreview.net/pdf?id=RF5Lb6NaZp" ftarget="_blank">PDF</a>]
    
    </div>

    <div class="abstract hidden">
    <p>
Extreme multi-label classification (XMC) is a popular framework for solving many real-world problems that require accurate prediction from a very large number of potential output choices. A popular approach for dealing with the large label space is to arrange the labels into a shallow tree-based index and then learn an ML model to efficiently search this index via beam search. Existing methods initialize the tree index by clustering the label space into a few mutually exclusive clusters based on pre-defined features and keep it fixed throughout the training procedure. This approach results in a sub-optimal indexing structure over the label space and limits the search performance to the quality of choices made during the initialization of the index. In this paper, we propose a novel method ELIAS which relaxes the tree-based index to a specialized weighted graph-based index which is learned end-to-end with the final task objective. More specifically, ELIAS models the discrete cluster-to-label assignments in the existing tree-based index as soft learnable parameters that are learned jointly with the rest of the ML model. ELIAS achieves state-of-the-art performance on several large-scale extreme classification benchmarks with millions of labels. In particular, ELIAS can be up to 2.5% better at precision@1 and up to 4% better at recall@100 than existing XMC methods. A PyTorch implementation of ELIAS along with other resources is available at https://github.com/nilesh2797/ELIAS.
</p>

    </div>


   </div> 
  </div>
  </li></ol>




  <ol class="bibliography"><li><div class="row">
  <div id="drone21" class="col-sm-10">
      <div class="title">DRONE: Data-aware Low-rank Compression for Large NLP Models</div>
      <div class="author">
                <em>Patrick H. Chen</em>,  
                Hsiang-fu Yu,
                Inderjit S. Dhillon,
                and Cho-jui, Hsieh
      </div>
      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
          
        (<b>NeurIPS</b>),
      
        2021
      </div>
        
    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="https://proceedings.neurips.cc/paper/2021/file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf" ftarget="_blank">PDF</a>]
    
    </div>

    <div class="abstract hidden">
    <p>
The representations learned by large-scale NLP models such as BERT have been
widely used in various tasks. However, the increasing model size of the pre-trained
models also brings efficiency challenges, including inference speed and model
size when deploying models on mobile devices. Specifically, most operations
in BERT consist of matrix multiplications. These matrices are not low-rank and
thus canonical matrix decompositions do not lead to efficient approximations. In
this paper, we observe that the learned representation of each layer lies in a low-dimensional space. Based on this observation, we propose DRONE (data-aware
low-rank compression), a provably optimal low-rank decomposition of weight
matrices, which has a simple closed form solution that can be efficiently computed.
DRONE can be applied to both fully-connected and self-attention layers appearing
in the BERT model. In addition to compressing standard models, our method
can also be used on distilled BERT models to further improve the compression
rate. Experimental results show that DRONE is able to improve both model size
and inference speed with limited loss in accuracy. Specifically, DRONE alone
achieves 1.92x speedup on the MRPC task with only 1.5% loss in accuracy, and
when DRONE is combined with distillation, it further achieves over 12.3x speedup
on various natural language inference tasks.
</p>

    </div>


   </div> 
  </div>
  </li></ol>




  <ol class="bibliography"><li><div class="row">
  <div id="canter20" class="col-sm-10">
      <div class="title">Clustering and Constructing User Coresets to Accelerate Large-scale Top-ùêæ Recommender Systems </div>
      <div class="author">
                Jyun-yu Jiang*,
                <em>Patrick H. Chen*</em>,  
                Cho-jui, Hsieh, 
                and Wei Wang
      </div>
      <div class="periodical">
      
        <em>In International World Wide Web Conference</em>
          
        (<b>WWW</b>),
      
        2020
      </div>
        
    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="http://web.cs.ucla.edu/~chohsieh/papers/cantor_www20.pdf" ftarget="_blank">PDF</a>]
    
    </div>

    <div class="abstract hidden">
    <p>
Top-K recommender systems aim to generate few but satisfactory
personalized recommendations for various practical applications,
such as item recommendation for e-commerce and link prediction
for social networks. However, the numbers of users and items can
be enormous, thereby leading to myriad potential recommendations
as well as the bottleneck in evaluating and ranking all possibilities.
Existing Maximum Inner Product Search (MIPS) based methods
treat the item ranking problem for each user independently and
the relationship between users has not been explored. In this paper,
we propose a novel model for clustering and navigating for top-K
recommenders (CANTOR) to expedite the computation of top-K
recommendations based on latent factor models. A clustering-based
framework is first presented to leverage user relationships to partition users into affinity groups, each of which contains users with
similar preferences. CANTOR then derives a coreset of representative vectors for each affinity group by constructing a set cover with
a theoretically guaranteed difference to user latent vectors. Using
these representative vectors in the coreset, approximate nearest
neighbor search is then applied to obtain a small set of candidate
items for each affinity group to be used when computing recommendations for each user in the affinity group. This approach can
significantly reduce the computation without compromising the
quality of the recommendations. Extensive experiments are conducted on six publicly available large-scale real-world datasets for
item recommendation and personalized link prediction. The experimental results demonstrate that CANTOR significantly speeds up
matrix factorization models with high precision. For instance, CANTOR can achieve 355.1x speedup for inferring recommendations
in a million-user network with 99.5% precision@1 to the original
system while the state-of-the-art method can only obtain 93.7x
speedup with 99.0% precision@1.
</p>

    </div>


   </div> 
  </div>
  </li></ol>


  <ol class="bibliography"><li><div class="row">
  <div id="tacl19" class="col-sm-10">
      <div class="title">Efficient Contextual Representation Learning With Continuous Outputs</div>
      <div class="author">
                Liunian Harold Li,
                <em>Patrick H. Chen</em>,  
                Cho-jui, Hsieh,
                and Kai-wei Chang
      </div>
      <div class="periodical">
      
        <em>In Transactions of the Association for Computational Linguistics</em>
          
        (<b>TACL</b>),
      
        2019
      </div>
        
    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="https://arxiv.org/pdf/1902.11269.pdf" ftarget="_blank">PDF</a>]
    
    </div>

    <div class="abstract hidden">
    <p>
Contextual representation models have
achieved great success in improving various downstream tasks. However, these
language-model-based encoders are difficult to train due to the large parameter sizes
and high computational complexity. By
carefully examining the training procedure,
we find that the softmax layer (the output
layer) causes significant inefficiency due
to the large vocabulary size. Therefore,
we redesign the learning objective and
propose an efficient framework for training
contextual representation models. Specifically, the proposed approach bypasses
the softmax layer by performing language
modeling with dimension reduction, and
allows the models to leverage pre-trained
word embeddings. Our framework reduces
the time spent on the output layer to a
negligible level, eliminates almost all the
trainable parameters of the softmax layer
and performs language modeling without
truncating the vocabulary. When applied
to ELMo, our method achieves a 4 times
speedup and eliminates 80% trainable
parameters while achieving competitive
performance on downstream tasks.
</p>

    </div>


   </div> 
  </div>
  </li></ol>



  <ol class="bibliography"><li><div class="row">
  <div id="screen19" class="col-sm-10">
      <div class="title">Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks</div>
      <div class="author">
                <em>Patrick H. Chen</em>,  
                Si Si,
                Sanjiv Kumar,
                Yang Li, 
                and Cho-jui, Hsieh,
      </div>
      <div class="periodical">
      
        <em>In International Conference on Learning Representations</em>
          
        (<b>ICLR</b>),
      
        2019
      </div>
        
    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="https://openreview.net/pdf?id=ByeMB3Act7" ftarget="_blank">PDF</a>]
    
    </div>

    <div class="abstract hidden">
    <p>
Neural language models have been widely used in various NLP tasks, including
machine translation, next word prediction and conversational agents. However, it
is challenging to deploy these models on mobile devices due to their slow prediction speed, where the bottleneck is to compute top candidates in the softmax
layer. In this paper, we introduce a novel softmax layer approximation algorithm
by exploiting the clustering structure of context vectors. Our algorithm uses a
light-weight screening model to predict a much smaller set of candidate words
based on the given context, and then conducts an exact softmax only within that
subset. Training such a procedure end-to-end is challenging as traditional clustering methods are discrete and non-differentiable, and thus unable to be used with
back-propagation in the training process. Using the Gumbel softmax, we are able
to train the screening model end-to-end on the training set to exploit data distribution. The algorithm achieves an order of magnitude faster inference than the
original softmax layer for predicting top-k words in various tasks such as beam
search in machine translation or next words prediction. For example, for machine translation task on German to English dataset with around 25K vocabulary,
we can achieve 20.4 times speed up with 98.9% precision@1 and 99.3% precision@5 with the original softmax layer prediction, while state-of-the-art (Zhang
et al., 2018) only achieves 6.7x speedup with 98.7% precision@1 and 98.1% precision@5 for the same task.
</p>

    </div>


   </div> 
  </div>
  </li></ol>


  <h2>Machine Learning Model Compression</h2>
  <hr/>
  <br/>


  <ol class="bibliography"><li><div class="row">
  <div id="drone21" class="col-sm-10">
      <div class="title">DRONE: Data-aware Low-rank Compression for Large NLP Models</div>
      <div class="author">
                <em>Patrick H. Chen</em>,  
                Hsiang-fu Yu,
                Inderjit S. Dhillon,
                and Cho-jui, Hsieh
      </div>
      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
          
        (<b>NeurIPS</b>),
      
        2021
      </div>
        
    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="https://proceedings.neurips.cc/paper/2021/file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf" ftarget="_blank">PDF</a>]
    
    </div>

    <div class="abstract hidden">
    <p>
The representations learned by large-scale NLP models such as BERT have been
widely used in various tasks. However, the increasing model size of the pre-trained
models also brings efficiency challenges, including inference speed and model
size when deploying models on mobile devices. Specifically, most operations
in BERT consist of matrix multiplications. These matrices are not low-rank and
thus canonical matrix decompositions do not lead to efficient approximations. In
this paper, we observe that the learned representation of each layer lies in a low-dimensional space. Based on this observation, we propose DRONE (data-aware
low-rank compression), a provably optimal low-rank decomposition of weight
matrices, which has a simple closed form solution that can be efficiently computed.
DRONE can be applied to both fully-connected and self-attention layers appearing
in the BERT model. In addition to compressing standard models, our method
can also be used on distilled BERT models to further improve the compression
rate. Experimental results show that DRONE is able to improve both model size
and inference speed with limited loss in accuracy. Specifically, DRONE alone
achieves 1.92x speedup on the MRPC task with only 1.5% loss in accuracy, and
when DRONE is combined with distillation, it further achieves over 12.3x speedup
on various natural language inference tasks.
</p>

    </div>


   </div> 
  </div>
  </li></ol>


  <ol class="bibliography"><li><div class="row">
  <div id="mulcode19" class="col-sm-10">
      <div class="title">MulCode: A Multiplicative Multi-way Model for Compressing Neural Language Model</div>
      <div class="author">
                Yukun, Ma*
                <em>Patrick H. Chen*</em>,  
                and Cho-jui, Hsieh
      </div>
      <div class="periodical">
      
        <em>In Conference on Empirical Methods in Natural Language Processing</em>
          
        (<b>EMNLP</b>),
      
      
        2019
      </div>
        
    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="https://aclanthology.org/D19-1529.pdf" target="_blank">PDF</a>]
    
    </div>

    <div class="abstract hidden">
    <p>
It is challenging to deploy deep neural nets on
memory-constrained devices due to the explosion of numbers of parameters. Especially, the
input embedding layer and Softmax layer usually dominate the memory usage in an RNNbased language model. For example, input
embedding and Softmax matrices in IWSLT2014 German-to-English data set account for
more than 80% of the total model parameters. To compress these embedding layers, we
propose MulCode, a novel multi-way multiplicative neural compressor. MulCode learns
an adaptively created matrix and its multiplicative compositions. Together with a prior
weighted loss, MulCode is more effective than
the state-of-the-art compression methods. On
the IWSLT-2014 machine translation data set,
MulCode achieved 17 times compression rate
for the embedding and Softmax matrices, and
when combined with quantization technique,
our method can achieve 41.38 times compression rate with very little loss in performance.

</p>

    </div>


   </div> 
  </div>
  </li></ol>
<!--
    
  </div>
-->
  <ol class="bibliography"><li><div class="row">
  <div id="groupreduce18" class="col-sm-10">
    
      <div class="title">GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking</div>
      <div class="author">
        
                <em>Patrick H. Chen</em>,  
                Si Si,
                Yang Li,
                Ciprian Chelba
                and Cho-jui, Hsieh
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
          
        (<b>NeurIPS</b>),
      
      
        2018
        
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      [<a href="https://proceedings.neurips.cc/paper/2018/file/a2b8a85a29b2d64ad6f47275bf1360c6-Paper.pdf" target="_blank">PDF</a>] 
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
    <p>
Model compression is essential for serving large deep neural nets on devices with
limited resources or applications that require real-time responses. As a case study, a
neural language model usually consists of one or more recurrent layers sandwiched
between an embedding layer used for representing input tokens and a softmax
layer for generating output tokens. For problems with a very large vocabulary
size, the embedding and the softmax matrices can account for more than half of
the model size. For instance, the bigLSTM model achieves great performance
on the One-Billion-Word (OBW) dataset with around 800k vocabulary, and its
word embedding and softmax matrices use more than 6GBytes space, and are
responsible for over 90% of the model parameters. In this paper, we propose
GroupReduce, a novel compression method for neural language models, based
on vocabulary-partition (block) based low-rank matrix approximation and the
inherent frequency distribution of tokens (the power-law distribution of words).
The experimental results show our method can significantly outperform traditional
compression methods such as low-rank approximation and pruning. On the OBW
dataset, our method achieved 6.6 times compression rate for the embedding and
softmax matrices, and when combined with quantization, our method can achieve
26 times compression rate, which translates to a factor of 12.8 times compression
for the entire model with very little degradation in perplexity.
</p>

    </div>
    
  </div>


</div>
</li></ol>




  <h2>Fundamental Machine Learning </h2>
  <hr/>
  <br/>


  <ol class="bibliography"><li><div class="row">
  <div id="forgetting21" class="col-sm-10">
    
      <div class="title">Overcoming Catastrophic Forgetting by Bayesian Generative Regularization</div>
      <div class="author">
                <em>Patrick H. Chen</em>,  
                Wei Wei,
                Cho-jui, Hsieh,
                and Bo Dai
      </div>

      <div class="periodical">
      
        <em>In Conference of Machine Learning</em>
          
        (<b>ICML</b>),
      
      
        2021
        
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      <!--[<a href="https://ieeexplore.ieee.org/iel7/4234/5534602/08624521.pdf" target="_blank">PDF</a>] -->
    
      [<a href="http://proceedings.mlr.press/v139/chen21v/chen21v.pdf" target="_blank">PDF</a>]

     <!-- [<a href="https://github.com/ryoungj/optdom" target="_blank">Code</a>] -->
    </div>
    
    
    
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
    <p>
In this paper, we propose a new method to over-come catastrophic forgetting by adding generative regularization to Bayesian inference frame-work. Bayesian method provides a general frame-work for continual learning. We could further construct a generative regularization term for all given classification models by leveraging energy-based models and Langevin dynamic sampling to enrich the features learned in each task. By combining discriminative and generative loss together, we empirically show that the proposed method outperforms state-of-the-art methods on a variety of tasks, avoiding catastrophic forgetting in continual learning. In particular, the proposed method outperforms baseline methods over 15%on the Fashion-MNIST dataset and 10%on the CUB dataset.
</p>



    </div>
    
  </div>


</div>
</li></ol>





  <ol class="bibliography"><li><div class="row">
  <div id="signopt20" class="col-sm-10">
    
      <div class="title">Sign-OPT: A Query-Efficient Hard-label Adversarial Attack</div>
      <div class="author">
          Minhao Cheng*, Simranjit Singh*, <em> Patrick H. Chen </em>, Pin-Yu Chen, Sijia Liu and Cho-Jui Hsieh
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations</em>
          
        (<b>ICLR</b>),
      
        2020
        
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
      <!--[<a href="https://ieeexplore.ieee.org/iel7/4234/5534602/08624521.pdf" target="_blank">PDF</a>] -->
    
      [<a href="https://arxiv.org/pdf/1909.10773.pdf" target="_blank">PDF</a>]

     <!-- [<a href="https://github.com/ryoungj/optdom" target="_blank">Code</a>] -->
    </div>
    
    
    
    

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
    <p>
We study the most practical problem setup for evaluating adversarial robustness
of a machine learning system with limited access: the hard-label black-box attack setting for generating adversarial examples, where limited model queries
are allowed and only the decision is provided to a queried data input. Several
algorithms have been proposed for this problem but they typically require huge
amount (>20,000) of queries for attacking one example. Among them, one of
the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack
can be modeled as an optimization problem where the objective function can be
evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization
formulation but propose to directly estimate the sign of gradient at any direction
instead of the gradient itself, which enjoys the benefit of single query. Using
this single query oracle for retrieving sign of directional derivative, we develop
a novel query-efficient Sign-OPT approach for hard-label black-box attack. We
provide a convergence analysis of the new algorithm and conduct experiments
on several models on MNIST, CIFAR-10 and ImageNet. We find that Sign-OPT
attack consistently requires 5√ó to 10√ó fewer queries when compared to the current
state-of-the-art approaches, and usually converges to an adversarial example with
smaller perturbation.
</p>



    </div>
    
  </div>


</div>
</li></ol>




</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container" style="text-align: center;">
    <!-- &copy; Copyright 2022 Yangjun  Ruan. -->
    
    
    
    Last updated: April 15, 2022.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="common.js"></script>


</html>

