
<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Yangjun  Ruan | Publications</title>
<meta name="description" content="Yangjun's homepage
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/~yjruan/assets/img/favicon.ico">
<!-- 
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
 -->

<link rel="stylesheet" href="/~yjruan/assets/css/main.css">

<link rel="canonical" href="/~yjruan/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-154898229-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-154898229-1');
  </script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="index.html">
       Patrick H. Chen 
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="index.html">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="publication.index">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h2 class="post-title"><b>Publications</b></h2>
    <p class="post-description"></p>
  </header>

  <article>
    <p>* below denotes equal contribution</p>
<div class="publications">


  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
  </div> -->

  <div id="duan2022augment" class="col-sm-10">
    
      <div class="title">Augment with Care: Contrastive Learning for the Boolean Satisfiability Problem</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                
                  Haonan Duan*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Pashootan Vaezipoor*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Max B Paulus,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Chris J Maddison
                
              
            
          
        
      </div>

      <div class="periodical">
      
        
        <em>ArXiv Preprint</em>,
        
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://arxiv.org/pdf/2202.08396.pdf" target="_blank">PDF</a>]
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Supervised learning can improve the design of state-of-the-art solvers for combinatorial problems, but labelling large numbers of combinatorial instances is often impractical due to exponential worst-case complexity. Inspired by the recent success of contrastive pre-training for images, we conduct a scientific study of the effect of augmentation design on contrastive pre-training for the Boolean satisfiability problem. While typical graph contrastive pre-training uses label-agnostic augmentations, our key insight is that many combinatorial problems have well-studied invariances, which allow for the design of label-preserving augmentations. We find that label-preserving augmentations are critical for the success of contrastive pre-training. We show that our representations are able to achieve comparable test accuracy to fully-supervised learning while using only 1% of the labels. We also demonstrate that our representations are more transferable to larger problems from unseen domains.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div> -->

  <div id="ruan2022optdom" class="col-sm-10">
    
      <div class="title">Optimal Representations for Covariate Shift</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan*</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Yann Dubois*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Chris J Maddison
                
              
            
          
        
      </div>

      <div class="periodical">
      
          
            <em>In International Conference on Learning Representations</em>
          
        (<b>ICLR</b>),
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://arxiv.org/pdf/2201.00057.pdf" target="_blank">PDF</a>]
      
    
    
    
    
      [<a href="https://github.com/ryoungj/optdom" target="_blank">Code</a>]
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are \textitexactly the set of \textitall representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two components. First, a representation must remain discriminative for the task, i.e., some predictor must be able to simultaneously minimize the source and target risk. Second, the representationâ€™s marginal support needs to be the same across source and target. We make this practical by designing self-supervised learning methods that only use unlabelled data and augmentations to train robust representations. Our objectives achieve state-of-the-art results on DomainBed, and give insights into the robustness of recent methods, such as CLIP.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div> -->

  <div id="ruan2021mcbits" class="col-sm-10">
    
      <div class="title">Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan*</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Karen Ullrich*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Daniel Severo*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  James Townsend,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Ashish Khisti,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Arnaud Doucet,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Alireza Makhzani,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Chris J Maddison
                
              
            
          
        
      </div>

      <div class="periodical">
      
          
            <em>In International Conference on Machine Learning</em>
          
        (<b>ICML</b>),
      
      
        2021
      
      
        <b style="color:#B71C1C;">[Long talk]</b>
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://arxiv.org/pdf/2102.11086.pdf" target="_blank">PDF</a>]
      
    
    
    
    
      [<a href="https://github.com/ryoungj/mcbits" target="_blank">Code</a>]
    
    
      
      [<a href="/~yjruan/assets/pdf/poster/McBits_poster.pdf" target="_blank">Poster</a>]
      
    
    
      
      [<a href="/~yjruan/assets/pdf/slides/McBits_slides.pdf" target="_blank">Slides</a>]
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Latent variable models have been successfully applied in lossless compression with the bits-back coding algorithm. However, bits-back suffers from an increase in the bitrate equal to the KL divergence between the approximate posterior and the true posterior. In this paper, we show how to remove this gap asymptotically by deriving bits-back coding algorithms from tighter variational bounds. The key idea is to exploit extended space representations of Monte Carlo estimators of the marginal likelihood. Naively applied, our schemes would require more initial bits than the standard bits-back coder, but we show how to drastically reduce this additional cost with couplings in the latent space. When parallel architectures can be exploited, our coders can achieve better rates than bits-back with little additional cost. We demonstrate improved lossless compression rates in a variety of settings, especially in out-of-distribution or sequential data compression.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div> -->

  <div id="ruan2020zol2l" class="col-sm-10">
    
      <div class="title">Learning to Learn by Zeroth-Order Oracle</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Yuanhao Xiong,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Sashank Reddi,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Sanjiv Kumar,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Cho-Jui Hsieh
                
              
            
          
        
      </div>

      <div class="periodical">
      
          
            <em>In International Conference on Learning Representations</em>
          
        (<b>ICLR</b>),
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://openreview.net/pdf?id=ryxz8CVYDH" target="_blank">PDF</a>]
      
    
    
    
    
      [<a href="https://github.com/ryoungj/ZO-L2L" target="_blank">Code</a>]
    
    
    
      
      [<a href="/~yjruan/assets/pdf/slides/ZO-L2L_slides.pdf" target="_blank">Slides</a>]
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the learning to learn (L2L) framework, we cast the design of optimization algorithms as a machine learning problem and use deep neural networks to learn the update rules. In this paper, we extend the L2L framework to zeroth-order (ZO) optimization setting, where no explicit gradient information is available. Our learned optimizer, modeled as recurrent neural network (RNN), first approximates gradient by ZO gradient estimator and then produces parameter update utilizing the knowledge of previous iterations. To reduce high variance effect due to ZO gradient estimator, we further introduce another RNN to learn the Gaussian sampling rule and dynamically guide the query direction sampling. Our learned optimizer outperforms hand-designed algorithms in terms of convergence rate and final solution on both synthetic and practical ZO optimization tasks (in particular, the black-box adversarial attack task, which is one of the most widely used tasks of ZO optimization). We finally conduct extensive analytical experiments to demonstrate the effectiveness of our proposed optimizer.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div> -->

  <div id="ren2019fastspeech" class="col-sm-10">
    
      <div class="title">FastSpeech: Fast, Robust and Controllable Text to Speech</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                
                  Yi Ren*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan*</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Xu Tan,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Tao Qin,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Sheng Zhao,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  Zhou Zhao,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Tie-Yan Liu
                
              
            
          
        
      </div>

      <div class="periodical">
      
          
            <em>In Advances in Neural Information Processing Systems</em>
          
        (<b>NeurIPS</b>),
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://arxiv.org/pdf/1905.09263.pdf" target="_blank">PDF</a>]
      
    
    
    
    
    
      
      [<a href="/~yjruan/assets/pdf/poster/fastspeech_poster.pdf" target="_blank">Poster</a>]
      
    
    
    
    
      [<a href="https://speechresearch.github.io/fastspeech/" target="_blank">Demo</a>]
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (eg, Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (ie, some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
  </div> -->

  <div id="ren2019data" class="col-sm-10">
    
      <div class="title">Data transmission in mobile edge networks: Whether and where to compress?</div>
      <div class="author">
        
        
        
          

          
          
          
          

          
          
          
            
              
                
                  Jinke Ren*,
                
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                <em>Yangjun Ruan*</em>,  
              
            
          
        
          

          
          
          
          

          
          
          
            
              
                
                  and Guanding Yu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        
        <em>IEEE Communications Letters</em>,
        
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      [<a class="abstract button" role="button">Abstract</a>]
    
    
    
    
      
      [<a href="https://ieeexplore.ieee.org/iel7/4234/5534602/08624521.pdf" target="_blank">PDF</a>]
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Future multimedia communication systems aim to support high-rate data transmission and deliver millisecond-scale latency performance. To achieve this goal, this letter revisits the multimedia compression strategy in the framework of mobile edge computing from the perspective of latency minimization. The uniqueness of this letter is that the delay caused by both data compression/decompression and data transmission are considered. According to where to compress and decompress data, three different schemes are investigated and the end-to-end latency of each scheme is analyzed. We first present some comparative theoretical results on the latency performance of different schemes for given compression ratio. Then, we prove that the optimal compression ratio to minimize the end-to-end latency has a binary structure. This initial work reveals that it is optimal to transmit without compression in the case that the communication capacity is sufficiently large.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container" style="text-align: center;">
    <!-- &copy; Copyright 2022 Yangjun  Ruan. -->
    
    
    
    Last updated: February 23, 2022.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="common.js"></script>


</html>

